import re
import unicodedata
import emoji
import sys
import os
import pandas as pd
from teencode_converter import TeencodeConverter

# --- SETUP M√îI TR∆Ø·ªúNG ---
sys.stdout.reconfigure(encoding='utf-8')
os.environ["JAVA_HOME"] = r"C:\Program Files\Java\jdk-21"
from py_vncorenlp import VnCoreNLP

BASE_DIR = r"K:\GithubRepo\comment-classification\src\vncorenlp"
if not os.path.exists(BASE_DIR):
    raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c VnCoreNLP t·∫°i: {BASE_DIR}")

vncorenlp = VnCoreNLP(annotators=["wseg", "pos"], save_dir=BASE_DIR)


class TextPreprocessor:
    def __init__(self, vncorenlp_instance=None, teencode_path="teencode.json"):
        self.vncorenlp = vncorenlp_instance
        self.teencode_converter = TeencodeConverter(teencode_path)

    def normalize_unicode(self, text):
        if not isinstance(text, str): return str(text)
        return unicodedata.normalize('NFC', text)

    def to_lower(self, text):
        return text.lower()

    def remove_urls(self, text):
        return re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    def standardize_punctuation(self, text):
        text = re.sub(r'!+', ' ! ', text)
        text = re.sub(r'\?+', ' ? ', text)
        text = re.sub(r'\.+', ' . ', text)
        text = re.sub(r',+', ' , ', text)
        return text

    def remove_duplicate_characters(self, text):
        return re.sub(r'(.)\1{2,}', r'\1', text)

    def normalize_teencode(self, text):
        return self.teencode_converter.replace(text)

    # --- S·ª¨A ƒê·ªîI QUAN TR·ªåNG: MASKING KH√îNG D√ôNG G·∫†CH D∆Ø·ªöI ---
    def mask_emojis(self, text):
        found_emojis = []
        
        def replace_callback(char, data_dict):
            demojized = emoji.demojize(char, delimiters=(" :", ": "))
            found_emojis.append(demojized)
            # D√πng m√£ EMOJITOKEN li·ªÅn m·∫°ch (coi nh∆∞ 1 t·ª´ ti·∫øng Anh)
            # VnCoreNLP s·∫Ω kh√¥ng c·∫Øt ch·ªØ n√†y ra.
            return f" EMOJITOKEN{len(found_emojis)-1} "

        masked_text = emoji.replace_emoji(text, replace=replace_callback)
        return masked_text, found_emojis

    # --- S·ª¨A ƒê·ªîI QUAN TR·ªåNG: RESTORE ƒê∆†N GI·∫¢N H∆†N ---
    def restore_emojis(self, text, found_emojis):
        """
        Kh√¥i ph·ª•c emoji t·ª´ m√£ EMOJITOKEN
        """
        # ƒê·ªÅ ph√≤ng tr∆∞·ªùng h·ª£p VnCoreNLP t√°ch s·ªë ra kh·ªèi ch·ªØ (VD: EMOJITOKEN 0)
        # Ta d√πng regex ƒë·ªÉ t√¨m: EMOJITOKEN + (kho·∫£ng tr·∫Øng t√πy √Ω) + S·ªë
        def restore_callback(match):
            idx = int(match.group(1))
            if 0 <= idx < len(found_emojis):
                return " " + found_emojis[idx] + " "
            return match.group(0) # N·∫øu l·ªói index th√¨ gi·ªØ nguy√™n

        # T√¨m t·∫•t c·∫£ pattern EMOJITOKEN + s·ªë
        text = re.sub(r'EMOJITOKEN\s*(\d+)', restore_callback, text)
        return text

    def segment_text(self, text):
        if self.vncorenlp:
            try:
                sentences = self.vncorenlp.word_segment(text)
                return ' '.join(sentences)
            except Exception as e:
                print(f"L·ªói Segment: {e}")
                return text
        return text

    def process(self, text):
        if not isinstance(text, str): return ""
        
        # 1. Pipeline l√†m s·∫°ch
        text = self.normalize_unicode(text)
        text = self.to_lower(text)
        text = self.remove_urls(text)
        text = self.remove_duplicate_characters(text)
        text = self.standardize_punctuation(text)
        text = self.normalize_teencode(text) 
        
        # 2. ·∫®n Emoji (Masking) b·∫±ng t·ª´ kh√≥a an to√†n EMOJITOKEN
        text, emoji_storage = self.mask_emojis(text)
        
        # 3. T√°ch t·ª´
        # VnCoreNLP th·∫•y "EMOJITOKEN0" s·∫Ω coi l√† t√™n ri√™ng (Np) ho·∫∑c t·ª´ l·∫° -> Gi·ªØ nguy√™n
        text = self.segment_text(text)
        
        # 4. Tr·∫£ l·∫°i Emoji
        text = self.restore_emojis(text, emoji_storage)
        
        # 5. X√≥a kho·∫£ng tr·∫Øng th·ª´a
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

# --- MAIN ---
if __name__ == "__main__":    
    print("--- B·∫Øt ƒë·∫ßu x·ª≠ l√Ω ---")
    current_dir = os.path.dirname(os.path.abspath(__file__))
    teencode_path_absolute = os.path.join(current_dir, "teencode.json")
    
    processor = TextPreprocessor(
        vncorenlp_instance=vncorenlp, 
        teencode_path=teencode_path_absolute
    )

    # Test v·ªõi chu·ªói c·ª©ng tr∆∞·ªõc ƒë·ªÉ ƒë·∫£m b·∫£o logic ƒë√∫ng
    test_str = "ai t·ª´ nh√≥m VSTAR qua ƒë√¢y ko, ƒëo√†n m√¨nh ·∫•y, xin l·ªói anh Q√∫y ü§£üôÉ"
    print(f"\nTEST NHANH:\nInput: {test_str}")
    print(f"Output: {processor.process(test_str)}\n")

    # X·ª≠ l√Ω file CSV
    inputfile = r"K:\GithubRepo\comment-classification\data\IzSYlr3VI1A_raw.csv"
    
    data = None
    for enc in ['utf-8-sig', 'utf-8', 'utf-16']:
        try:
            data = pd.read_csv(inputfile, encoding=enc)
            # Check nhanh
            str(data['text'].iloc[0]) 
            break
        except Exception:
            continue
            
    if data is not None:
        data = data.dropna(subset=['text']) 
        lim = 6
        cur = 0
        print("-" * 40)
        for input_text in data["text"]:
            input_str = str(input_text)
            # B·ªè qua n·∫øu d√≤ng qu√° ng·∫Øn ho·∫∑c r·ªóng
            if not input_str.strip(): continue

            output_text = processor.process(input_str)
            print(f"Input:  {input_str}")
            print(f"Output: {output_text}")
            print("-" * 40)
            cur += 1
            if cur >= lim: break
    else:
        print("Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file CSV.")